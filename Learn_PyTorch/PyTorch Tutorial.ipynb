{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7acc8ca",
   "metadata": {},
   "source": [
    "# PyTorch Tutorial\n",
    "\n",
    "**William Yue**\n",
    "\n",
    "In this Jupyter notebook, my goal is to gain familiarity with PyTorch by following the [online tutorials](https://pytorch.org/tutorials/). Hopefully I will know how it works at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b56f409",
   "metadata": {},
   "source": [
    "## Introduction to PyTorch\n",
    "\n",
    "### Tensors\n",
    "\n",
    "Let's start by getting `torch` and `numpy` in here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "822745ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be15c71",
   "metadata": {},
   "source": [
    "Tensors appear to just be `torch`'s version of a matrix or multi-dimensional array, similar to `numpy`'s ndarrays. The difference is that tensors can run on GPUs or other fast hardware. They are also optimized for automatic differentiation.\n",
    "\n",
    "#### Initializing a Tensor\n",
    "\n",
    "There are several ways to make a tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "033f797a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n"
     ]
    }
   ],
   "source": [
    "data = [[1,2,3],[4,5,6]]\n",
    "x_data = torch.tensor(data)\n",
    "print(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba761585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "np_data = np.arange(6).reshape(2,3)\n",
    "x_np = torch.tensor(np_data)\n",
    "print(x_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7455102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1],\n",
      "        [1, 1, 1]])\n",
      "tensor([[0.7469, 0.3134, 0.7703],\n",
      "        [0.1933, 0.7184, 0.0972]])\n"
     ]
    }
   ],
   "source": [
    "x_ones = torch.ones_like(x_data)\n",
    "print(x_ones)\n",
    "\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float)\n",
    "print(x_rand)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61c4501",
   "metadata": {},
   "source": [
    "Note that we have a problem if we don't convert the `dtype` in the `torch.rand_like` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "573fd5cc",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "\"check_uniform_bounds\" not implemented for 'Long'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8648/2854645661.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mx_rand_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrand_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: \"check_uniform_bounds\" not implemented for 'Long'"
     ]
    }
   ],
   "source": [
    "x_rand_test = torch.rand_like(x_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34295942",
   "metadata": {},
   "source": [
    "This appears to be because the initial tenesor `x_data` has the datatype `Long` (64-bit integer, according to [documentation](https://pytorch.org/docs/stable/tensor_attributes.html)), and there is no way to sample a random number in the interval `[0,1)` for this datatype.\n",
    "\n",
    "We can also directly specify the shape for `torch.rand`, `torch.ones`, and `torch.zeros`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2bbc4cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7427, 0.3731, 0.1832],\n",
      "        [0.7499, 0.2047, 0.2797]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "shape=(2,3)\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "\n",
    "print(rand_tensor,ones_tensor,zeros_tensor,sep='\\n')\n",
    "\n",
    "another_ones_tensor = torch.ones(4,5)\n",
    "print(another_ones_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4edc424",
   "metadata": {},
   "source": [
    "There are several attributes of tensors that we can check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9bb0475d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4])\n",
      "torch.float32\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand(3,4)\n",
    "\n",
    "print(tensor.shape)\n",
    "print(tensor.dtype)\n",
    "print(tensor.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3160191",
   "metadata": {},
   "source": [
    "#### Operations on Tensors\n",
    "\n",
    "If you're observant, you'll notice that the device above that the tensor is stored on is a CPU! It turns out that by default, all tensors are initialized with `cpu` as their device. I'm on a Makerspace computer that comes with a NVIDIA GPU that can use CUDA, so we'll want to convert the device to a GPU possible. We can first check if CUDA is available before switching the tensor to that using the `.to` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2bbb014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2022, 0.7543, 0.1385, 0.5809],\n",
      "        [0.0607, 0.5798, 0.9910, 0.2934],\n",
      "        [0.2830, 0.0540, 0.9153, 0.2379]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    tensor = tensor.to('cuda')\n",
    "\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a816e089",
   "metadata": {},
   "source": [
    "Looks better now!\n",
    "\n",
    "Tensors can be operated on similar to `numpy` arrays, with standard indexing and slicing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6da2dea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3103, 0.9059, 0.9750, 0.2465],\n",
      "        [0.1227, 0.3771, 0.5851, 0.5742],\n",
      "        [0.5704, 0.9578, 0.3762, 0.6578],\n",
      "        [0.5233, 0.2576, 0.3134, 0.2994]])\n",
      "tensor([0.3103, 0.9059, 0.9750, 0.2465])\n",
      "tensor([[0.3103, 0.9059, 0.9750, 0.2465]])\n",
      "tensor([0.3103, 0.1227, 0.5704, 0.5233])\n",
      "tensor([0.2465, 0.5742, 0.6578, 0.2994])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand(4,4)\n",
    "print(tensor)\n",
    "\n",
    "print(tensor[0])\n",
    "print(tensor[0:1])\n",
    "print(tensor[:,0])\n",
    "print(tensor[:,-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cc0e8a",
   "metadata": {},
   "source": [
    "Note that similar to `numpy` arrays, `tensor[0]` and `tensor[0:1]` have different dimensionalities. We can also do standard concatenation along a given `dim` (not `axis`) using `torch.cat`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7b6e8d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3103, 0.9059, 0.9750, 0.2465],\n",
      "        [0.1227, 0.3771, 0.5851, 0.5742],\n",
      "        [0.5704, 0.9578, 0.3762, 0.6578],\n",
      "        [0.5233, 0.2576, 0.3134, 0.2994],\n",
      "        [0.3103, 0.9059, 0.9750, 0.2465],\n",
      "        [0.1227, 0.3771, 0.5851, 0.5742],\n",
      "        [0.5704, 0.9578, 0.3762, 0.6578],\n",
      "        [0.5233, 0.2576, 0.3134, 0.2994],\n",
      "        [0.3103, 0.9059, 0.9750, 0.2465],\n",
      "        [0.1227, 0.3771, 0.5851, 0.5742],\n",
      "        [0.5704, 0.9578, 0.3762, 0.6578],\n",
      "        [0.5233, 0.2576, 0.3134, 0.2994]])\n",
      " ----- \n",
      "tensor([[0.3103, 0.9059, 0.9750, 0.2465, 0.3103, 0.9059, 0.9750, 0.2465, 0.3103,\n",
      "         0.9059, 0.9750, 0.2465],\n",
      "        [0.1227, 0.3771, 0.5851, 0.5742, 0.1227, 0.3771, 0.5851, 0.5742, 0.1227,\n",
      "         0.3771, 0.5851, 0.5742],\n",
      "        [0.5704, 0.9578, 0.3762, 0.6578, 0.5704, 0.9578, 0.3762, 0.6578, 0.5704,\n",
      "         0.9578, 0.3762, 0.6578],\n",
      "        [0.5233, 0.2576, 0.3134, 0.2994, 0.5233, 0.2576, 0.3134, 0.2994, 0.5233,\n",
      "         0.2576, 0.3134, 0.2994]])\n"
     ]
    }
   ],
   "source": [
    "t0 = torch.cat([tensor]*3, dim=0)\n",
    "t1 = torch.cat([tensor]*3, dim=1)\n",
    "\n",
    "print(t0,t1,sep='\\n ----- \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625ddbb6",
   "metadata": {},
   "source": [
    "We also have standard arithmetic operations. Three ways to do matrix multiplication are shown below, using `@` and `matmul`. `y1`, `y2`, and `y3` should have the same value. Note that `.T` transposes the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "25e30a1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.9284, 1.0916, 1.5737, 0.7751],\n",
       "        [1.0916, 0.8293, 1.0290, 0.5166],\n",
       "        [1.5737, 1.0290, 1.8171, 0.8601],\n",
       "        [0.7751, 0.5166, 0.8601, 0.5281]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1 = tensor @ tensor.T\n",
    "y2 = tensor.matmul(tensor.T)\n",
    "y3 = torch.rand(tensor.shape)\n",
    "torch.matmul(tensor, tensor.T, out=y3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41b6cf5",
   "metadata": {},
   "source": [
    "If you want to do element-wise multiplication instead, you can use `*` or `mul` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "de77f76b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0963, 0.8207, 0.9507, 0.0607],\n",
       "        [0.0151, 0.1422, 0.3423, 0.3297],\n",
       "        [0.3254, 0.9174, 0.1416, 0.4327],\n",
       "        [0.2739, 0.0664, 0.0982, 0.0896]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z1 = tensor * tensor\n",
    "z2 = tensor.mul(tensor)\n",
    "z3 = torch.rand(tensor.shape) # note that using torch.rand_like also works\n",
    "torch.mul(tensor, tensor, out=z3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3b4cf3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
